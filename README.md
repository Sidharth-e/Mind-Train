# Mind Train ğŸ§ 

A personal AI assistant fine-tuned on TinyLlama to answer questions about **Sidharth E** - a Full Stack Developer from Kerala, India.

## Overview

This project fine-tunes the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model using LoRA (Low-Rank Adaptation) to create a personalized AI chatbot that can answer questions about my professional background, skills, experience, and projects.

## Features

- **Personalized AI Assistant**: Trained on 265+ Q&A pairs covering professional information
- **Efficient Fine-tuning**: Uses 4-bit quantization and LoRA for memory-efficient training
- **GPU Optimized**: Runs on consumer GPUs (tested on RTX 3050 Ti 4GB)
- **Clean Output**: Includes response cleaning to remove training artifacts

## Tech Stack

| Component    | Technology               |
| ------------ | ------------------------ |
| Base Model   | TinyLlama-1.1B-Chat-v1.0 |
| Fine-tuning  | LoRA with PEFT           |
| Quantization | BitsAndBytes (4-bit NF4) |
| Framework    | Transformers, PyTorch    |

## Project Structure

```
Mind Train/
â”œâ”€â”€ train.py              # Training script with dataset and model configuration
â”œâ”€â”€ chat.py               # Interactive chat interface for the fine-tuned model
â”œâ”€â”€ Sidharth_AI_Model/    # Saved LoRA adapter weights
â”œâ”€â”€ outputs_backup/       # Training checkpoints
â””â”€â”€ unsloth_compiled_cache/
```

## Requirements

- Python 3.10+
- CUDA-compatible GPU (4GB+ VRAM recommended)
- PyTorch with CUDA support

### Dependencies

```bash
pip install torch transformers datasets peft bitsandbytes accelerate
```

## Usage

### Training the Model

Fine-tune TinyLlama with your custom dataset:

```bash
python train.py
```

**Training Configuration:**

- Epochs: 15
- Learning Rate: 2e-4
- Batch Size: 1 (with 4 gradient accumulation steps)
- LoRA Rank: 16
- Target Modules: q_proj, k_proj, v_proj, o_proj

### Running the Chatbot

Start an interactive chat session with your fine-tuned model:

```bash
python chat.py
```

**Example interaction:**

```
ğŸ¤– Sidharth AI Ready. Type 'exit' to quit.
==================================================

You: Who is Sidharth E?
AI: Sidharth E is a Full Stack Developer based in Kerala, India with nearly 5 years of experience building scalable web applications. He specializes in React, Node.js, Next.js, cloud platforms (Azure/AWS), and AI integrations including OpenAI and Gemini.

You: What are his skills?
AI: Sidharth's skills span: Frontend (React, Next.js, TypeScript, Tailwind), Backend (Node.js, Python, FastAPI), Databases (MongoDB, Cosmos DB), Cloud (Azure, AWS, Jenkins), AI/ML (OpenAI, Gemini, Langchain, HuggingFace), and Tools (Git, VS Code, Cursor).
```

## Training Data

The model is trained on a curated dataset covering:

- ğŸ§‘â€ğŸ’¼ **Identity & Introduction** - Basic information about Sidharth E
- ğŸ’¼ **Work Experience** - KellyOCG, Poumki Digital, TCS
- ğŸ› ï¸ **Technical Skills** - Frontend, Backend, Cloud, AI/ML, Databases
- ğŸ“¦ **Projects** - GRACE Sync, Project Convergence, e-slide
- ğŸ“ **Education** - BCA, MCA (AI/ML)
- ğŸ† **Awards** - Star of the Month at TCS
- ğŸ“ **Contact Information** - Email, phone, portfolio

## Model Architecture

```
Base Model: TinyLlama-1.1B-Chat-v1.0
â”œâ”€â”€ Quantization: 4-bit (NF4)
â”œâ”€â”€ LoRA Config:
â”‚   â”œâ”€â”€ Rank (r): 16
â”‚   â”œâ”€â”€ Alpha: 32
â”‚   â”œâ”€â”€ Dropout: 0.05
â”‚   â””â”€â”€ Target: [q_proj, k_proj, v_proj, o_proj]
â””â”€â”€ Prompt Format: Alpaca
```

## Customization

To train your own version:

1. **Modify the dataset** in `train.py`:

   ```python
   raw_data = [
       {"text": "Your question here?", "output": "Your answer here."},
       # Add more Q&A pairs...
   ]
   ```

2. **Adjust training parameters** as needed:

   ```python
   num_train_epochs=15,    # Increase for better learning
   learning_rate=2e-4,     # Adjust based on dataset size
   ```

3. **Run training** and the adapter will be saved to `Sidharth_AI_Model/`

## Performance Tips

- **Memory**: Uses ~4GB VRAM with 4-bit quantization
- **Speed**: Training takes ~10-15 minutes on RTX 3050 Ti
- **Quality**: More diverse Q&A pairs = better generalization

## License

This project is for personal/educational use.

## Author

**Sidharth E**  
Full Stack Developer | Kerala, India  
ğŸ“§ sidharthe38943@gmail.com  
ğŸŒ [sidharthe.dev](https://sidharthe.dev)
